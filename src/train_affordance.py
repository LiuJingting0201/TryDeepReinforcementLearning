import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset, random_split
from torchvision import transforms
import numpy as np
from PIL import Image
import json
from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
import glob
from torch.cuda.amp import autocast, GradScaler  # æ··åˆç²¾åº¦è®­ç»ƒ


class AffordanceDataset(Dataset):
    def __init__(self, data_dir, is_train=True, oversample_positives=True):
        self.data_dir = os.path.join(data_dir, 'train' if is_train else 'test')
        self.rgb_paths = sorted(glob.glob(os.path.join(self.data_dir, '*_rgb.png')))
        self.depth_paths = sorted(glob.glob(os.path.join(self.data_dir, '*_depth.npy')))
        self.afford_paths = sorted(glob.glob(os.path.join(self.data_dir, '*_affordance.npy')))
        self.angle_paths = sorted(glob.glob(os.path.join(self.data_dir, '*_angles.npy')))
        self.transform = transforms.ToTensor()
        self.num_angle_classes = 36  # æ¯10åº¦ä¸€ç±»ï¼Œ0-35å¯¹åº”0Â°-350Â°

        # è¿‡é‡‡æ ·æ­£æ ·æœ¬åœºæ™¯
        if is_train and oversample_positives:
            self._oversample_positive_scenes()

    def _oversample_positive_scenes(self):
        """è¿‡é‡‡æ ·åŒ…å«æ­£affordanceåƒç´ çš„åœºæ™¯"""
        original_indices = list(range(len(self.rgb_paths)))
        positive_indices = []

        # æ‰¾åˆ°åŒ…å«æ­£æ ·æœ¬çš„åœºæ™¯
        for idx in original_indices:
            afford = np.load(self.afford_paths[idx])
            if np.sum(afford > 0.5) > 0:  # åŒ…å«æ­£æ ·æœ¬
                positive_indices.append(idx)

        # è¿‡é‡‡æ ·æ­£æ ·æœ¬åœºæ™¯ (å¤åˆ¶3å€)
        oversampled_indices = original_indices + positive_indices * 3

        # æ›´æ–°è·¯å¾„åˆ—è¡¨
        self.rgb_paths = [self.rgb_paths[i] for i in oversampled_indices]
        self.depth_paths = [self.depth_paths[i] for i in oversampled_indices]
        self.afford_paths = [self.afford_paths[i] for i in oversampled_indices]
        self.angle_paths = [self.angle_paths[i] for i in oversampled_indices]

        print(f"   è¿‡é‡‡æ ·åè®­ç»ƒåœºæ™¯: {len(self.rgb_paths)} (åŸå§‹: {len(original_indices)}, æ­£æ ·æœ¬åœºæ™¯: {len(positive_indices)})")

    def __len__(self):
        return len(self.rgb_paths)

    def __getitem__(self, idx):
        rgb = Image.open(self.rgb_paths[idx]).convert('RGB')
        rgb = self.transform(rgb)  # (3,H,W)

        depth = np.load(self.depth_paths[idx])
        depth = torch.tensor(depth).unsqueeze(0).float()  # (1,H,W)

        afford = np.load(self.afford_paths[idx])
        afford = torch.tensor(afford).unsqueeze(0).float()  # (1,H,W)

        angle = np.load(self.angle_paths[idx])
        # ç¦»æ•£åŒ–è§’åº¦ï¼š0-360åº¦ -> 0-35ç±»
        angle_class = torch.tensor((angle / 10).astype(int) % self.num_angle_classes).long()  # (H,W)

        # åªå¯¹ affordance=1 çš„åƒç´ è®¡ç®—è§’åº¦æŸå¤±
        valid_angle_mask = (afford.squeeze(0) > 0.5).float()

        x = torch.cat([rgb, depth], dim=0)  # (4,H,W)
        return x, afford.squeeze(0), angle_class.squeeze(0), valid_angle_mask.squeeze(0)


class UNet(nn.Module):
    def __init__(self, in_channels=4, out_channels=37):  # 36è§’åº¦ç±» + 1 affordance
        super(UNet, self).__init__()
        # ç¼–ç å™¨ (Encoder) - ç‰¹å¾æå–
        self.enc1 = self.conv_block(in_channels, 64)    # 4 â†’ 64
        self.enc2 = self.conv_block(64, 128)            # 64 â†’ 128
        self.enc3 = self.conv_block(128, 256)           # 128 â†’ 256
        self.enc4 = self.conv_block(256, 512)           # 256 â†’ 512

        self.pool = nn.MaxPool2d(2)  # 2x2æ± åŒ–ï¼Œä¸‹é‡‡æ ·

        # è§£ç å™¨ (Decoder) - ç‰¹å¾é‡å»º
        self.dec3 = self.conv_block(512, 256)           # 512 â†’ 256
        self.dec2 = self.conv_block(256, 128)           # 256 â†’ 128
        self.dec1 = self.conv_block(128, 64)            # 128 â†’ 64

        # ä¸Šé‡‡æ · (Upsampling)
        self.upconv3 = nn.ConvTranspose2d(512, 256, 2, stride=2)  # 512 â†’ 256
        self.upconv2 = nn.ConvTranspose2d(256, 128, 2, stride=2)  # 256 â†’ 128
        self.upconv1 = nn.ConvTranspose2d(128, 64, 2, stride=2)   # 128 â†’ 64

        # æœ€ç»ˆè¾“å‡ºå±‚
        self.final = nn.Conv2d(64, out_channels, 1)     # 64 â†’ 37

    def conv_block(self, in_c, out_c):
        """å·ç§¯å—: Conv2d â†’ BatchNorm â†’ ReLU â†’ Conv2d â†’ BatchNorm â†’ ReLU"""
        return nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1),  # 3x3å·ç§¯ï¼Œä¿æŒå°ºå¯¸
            nn.BatchNorm2d(out_c),                  # æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True),                  # æ¿€æ´»å‡½æ•°
            nn.Conv2d(out_c, out_c, 3, padding=1),  # ç¬¬äºŒä¸ª3x3å·ç§¯
            nn.BatchNorm2d(out_c),                  # æ‰¹å½’ä¸€åŒ–
            nn.ReLU(inplace=True)                   # æ¿€æ´»å‡½æ•°
        )

    def forward(self, x):
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))

        d3 = self.upconv3(e4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)

        d2 = self.upconv2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)

        d1 = self.upconv1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

class UNetLarge(nn.Module):
    """æ›´æ·±æ›´å¤§çš„UNetï¼Œé€‚åˆ300åœºæ™¯è®­ç»ƒï¼Œ5å±‚æ¶æ„"""
    def __init__(self, in_channels=4, out_channels=37):
        super(UNetLarge, self).__init__()
        # æ›´å®½æ›´æ·±çš„é€šé“ (1.5xæ·±åº¦)
        self.enc1 = self.conv_block(in_channels, 96)    # 4 â†’ 96
        self.enc2 = self.conv_block(96, 192)            # 96 â†’ 192
        self.enc3 = self.conv_block(192, 384)           # 192 â†’ 384
        self.enc4 = self.conv_block(384, 768)           # 384 â†’ 768
        self.enc5 = self.conv_block(768, 1536)          # 768 â†’ 1536 (æ–°å¢ç¬¬5å±‚)

        self.pool = nn.MaxPool2d(2)

        # å¯¹åº”çš„è§£ç å™¨
        self.dec4 = self.conv_block(1536, 768)          # 1536 â†’ 768
        self.dec3 = self.conv_block(768, 384)           # 768 â†’ 384
        self.dec2 = self.conv_block(384, 192)           # 384 â†’ 192
        self.dec1 = self.conv_block(192, 96)            # 192 â†’ 96

        self.upconv4 = nn.ConvTranspose2d(1536, 768, 2, stride=2)  # 1536 â†’ 768
        self.upconv3 = nn.ConvTranspose2d(768, 384, 2, stride=2)   # 768 â†’ 384
        self.upconv2 = nn.ConvTranspose2d(384, 192, 2, stride=2)   # 384 â†’ 192
        self.upconv1 = nn.ConvTranspose2d(192, 96, 2, stride=2)    # 192 â†’ 96

        self.final = nn.Conv2d(96, out_channels, 1)

        # Dropout for regularization (æ›´å¼ºçš„æ­£åˆ™åŒ–)
        self.dropout = nn.Dropout2d(0.15)

    def conv_block(self, in_c, out_c):
        return nn.Sequential(
            nn.Conv2d(in_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True),
            nn.Conv2d(out_c, out_c, 3, padding=1),
            nn.BatchNorm2d(out_c),
            nn.ReLU(inplace=True)
        )

    def forward(self, x):
        # Encoder with 5 levels (æ›´æ·±)
        e1 = self.enc1(x)
        e2 = self.enc2(self.pool(e1))
        e3 = self.enc3(self.pool(e2))
        e4 = self.enc4(self.pool(e3))
        e5 = self.enc5(self.pool(e4))  # æ–°å¢ç¬¬5å±‚

        # Decoder with 5 levels (å¯¹åº”è§£ç )
        d4 = self.upconv4(e5)
        d4 = torch.cat([d4, e4], dim=1)
        d4 = self.dec4(d4)
        d4 = self.dropout(d4)  # æ›´å¼ºçš„dropout

        d3 = self.upconv3(d4)
        d3 = torch.cat([d3, e3], dim=1)
        d3 = self.dec3(d3)
        d3 = self.dropout(d3)

        d2 = self.upconv2(d3)
        d2 = torch.cat([d2, e2], dim=1)
        d2 = self.dec2(d2)
        d2 = self.dropout(d2)

        d1 = self.upconv1(d2)
        d1 = torch.cat([d1, e1], dim=1)
        d1 = self.dec1(d1)

        out = self.final(d1)
        return out


def calculate_metrics(pred_afford, true_afford, pred_angle, true_angle, valid_mask):
    """è®¡ç®—è®­ç»ƒæŒ‡æ ‡"""
    metrics = {}

    # Affordance metrics
    pred_afford_prob = torch.sigmoid(pred_afford)
    pred_afford_binary = (pred_afford_prob > 0.5).float()

    # Accuracy
    afford_acc = (pred_afford_binary == true_afford).float().mean()

    # Precision, Recall, F1 for positive class
    true_pos = (pred_afford_binary * true_afford).sum()
    pred_pos = pred_afford_binary.sum()
    actual_pos = true_afford.sum()

    precision = true_pos / (pred_pos + 1e-6)
    recall = true_pos / (actual_pos + 1e-6)
    f1 = 2 * precision * recall / (precision + recall + 1e-6)

    metrics['afford_acc'] = afford_acc.item()
    metrics['afford_precision'] = precision.item()
    metrics['afford_recall'] = recall.item()
    metrics['afford_f1'] = f1.item()

    # Angle metrics (åªå¯¹æœ‰æ•ˆåƒç´ )
    if valid_mask.sum() > 0:
        angle_pred_classes = pred_angle.argmax(dim=1)
        angle_acc = (angle_pred_classes == true_angle).float()
        angle_acc = (angle_acc * valid_mask).sum() / (valid_mask.sum() + 1e-6)
        metrics['angle_acc'] = angle_acc.item()
    else:
        metrics['angle_acc'] = 0.0

    return metrics


def train_model():
    data_dir = './data/affordance_v5'
    full_dataset = AffordanceDataset(data_dir, is_train=True)

    # 300åœºæ™¯: 90%è®­ç»ƒï¼Œ10%éªŒè¯
    train_size = int(0.9 * len(full_dataset))
    val_size = len(full_dataset) - train_size
    train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size], generator=torch.Generator().manual_seed(42))

    # æ›´å°batch_size + æ¢¯åº¦ç´¯ç§¯ (é˜²æ­¢OOMï¼Œæ¨¡æ‹Ÿæ›´å¤§batch)
    batch_size = 4  # ä»8é™åˆ°4ï¼Œé˜²æ­¢5å±‚æ·±ç½‘ç»œOOM
    accumulation_steps = 2  # æ¢¯åº¦ç´¯ç§¯2æ­¥ï¼Œç›¸å½“äºæœ‰æ•ˆbatch_size=8
    effective_batch_size = batch_size * accumulation_steps

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)

    # ä½¿ç”¨æ›´å¤§æ¨¡å‹
    model = UNetLarge().cuda()  # æ›´å¤§å®¹é‡æ¨¡å‹

    # åŠ æƒFocal Loss + Dice Loss å¤„ç†æåº¦ä¸å¹³è¡¡çš„affordanceæ ‡ç­¾
    class FocalLoss(nn.Module):
        def __init__(self, alpha=0.25, gamma=1.0, pos_weight=50.0):
            super().__init__()
            self.alpha = alpha
            self.gamma = gamma
            self.pos_weight = pos_weight

        def forward(self, inputs, targets):
            bce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction='none')
            pt = torch.exp(-bce_loss)

            # ç»™æ­£æ ·æœ¬æ›´é«˜çš„æƒé‡æ¥å¤„ç†æåº¦ä¸å¹³è¡¡
            weights = torch.where(targets == 1, self.pos_weight, 1.0)

            focal_loss = self.alpha * weights * (1 - pt) ** self.gamma * bce_loss
            return focal_loss.mean()

    class DiceLoss(nn.Module):
        def __init__(self, smooth=1.0):
            super().__init__()
            self.smooth = smooth

        def forward(self, inputs, targets):
            inputs = torch.sigmoid(inputs)

            # Flatten
            inputs = inputs.view(-1)
            targets = targets.view(-1)

            intersection = (inputs * targets).sum()
            dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)

            return 1 - dice

    # ç»„åˆæŸå¤±: Focal Loss + Dice Loss
    criterion_afford_focal = FocalLoss(alpha=0.25, gamma=1.0, pos_weight=50.0)
    criterion_afford_dice = DiceLoss()
    criterion_angle = nn.CrossEntropyLoss(reduction='none')

    # æ›´æ¿€è¿›çš„ä¼˜åŒ–å™¨è®¾ç½®
    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=1e-4, betas=(0.9, 0.999))
    scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=15, T_mult=2)  # æ›´é•¿çš„å‘¨æœŸ

    # æ··åˆç²¾åº¦è®­ç»ƒè®¾ç½®
    scaler = GradScaler()
    # æ¢¯åº¦è£å‰ªï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
    max_grad_norm = 1.0

    best_val_loss = float('inf')
    patience = 25  # æ›´é•¿çš„è€å¿ƒ (æ•°æ®å¤šï¼Œéœ€è¦æ›´å¤šè®­ç»ƒ)
    patience_counter = 0

    print("ğŸš€ å¼€å§‹è®­ç»ƒå¤§è§„æ¨¡å¯ä¾›æ€§æ¨¡å‹ (300åœºæ™¯)...")
    print(f"   è®­ç»ƒé›†: {len(train_dataset)} ä¸ªåœºæ™¯")
    print(f"   éªŒè¯é›†: {len(val_dataset)} ä¸ªåœºæ™¯")
    print(f"   æ¨¡å‹: UNetLarge (5å±‚, 4â†’96â†’192â†’384â†’768â†’1536 channels, å¢å¼ºå®¹é‡)")
    print(f"   ä¼˜åŒ–å™¨: AdamW (lr={optimizer.param_groups[0]['lr']:.1e}, wd=1e-4)")
    print(f"   æŸå¤±: åŠ æƒFocal+Dice Loss (afford, æ­£æ ·æœ¬50xæƒé‡) + Masked CE (angle)")
    print(f"   æ‰¹å¤§å°: {batch_size} (æ¢¯åº¦ç´¯ç§¯{accumulation_steps}æ­¥, æœ‰æ•ˆbatch={effective_batch_size})")
    print(f"   æ··åˆç²¾åº¦: AMPå¯ç”¨ï¼Œæ¢¯åº¦è£å‰ª={max_grad_norm}")
    print("=" * 80)

    for epoch in range(100):
        # ===== è®­ç»ƒé˜¶æ®µ =====
        model.train()
        train_loss = 0.0
        train_metrics = {'afford_acc': 0, 'afford_precision': 0, 'afford_recall': 0, 'afford_f1': 0, 'angle_acc': 0}

        for batch_idx, (x, afford, angle, valid_mask) in enumerate(train_loader):
            x, afford, angle, valid_mask = x.cuda(), afford.cuda(), angle.cuda(), valid_mask.cuda()

            # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
            with autocast():
                pred = model(x)
                pred_afford = pred[:, 0]    # (B, H, W)
                pred_angle = pred[:, 1:]    # (B, 36, H, W)

                # Affordance loss: Focal Loss + Dice Loss ç»„åˆ
                loss_afford_focal = criterion_afford_focal(pred_afford, afford)
                loss_afford_dice = criterion_afford_dice(pred_afford, afford)
                loss_afford = 0.7 * loss_afford_focal + 0.3 * loss_afford_dice  # 70% Focal + 30% Dice

                # Angle loss (åªå¯¹ affordance=1 çš„åƒç´ è®¡ç®—)
                if valid_mask.sum() > 0:
                    angle_loss_per_pixel = criterion_angle(pred_angle, angle)  # (B, H, W)
                    # åº”ç”¨mask: åªåœ¨æœ‰æ•ˆåƒç´ ä¸Šè®¡ç®—æŸå¤±
                    masked_angle_loss = angle_loss_per_pixel * valid_mask  # (B, H, W)
                    loss_angle = masked_angle_loss.sum() / (valid_mask.sum() + 1e-6)  # å½’ä¸€åŒ–
                else:
                    loss_angle = torch.tensor(0.0).cuda()

                # å¤šä»»åŠ¡æŸå¤±: å¤§æ•°æ®ä¸‹è°ƒæ•´æƒé‡
                loss = 5.0 * loss_afford + 1.0 * loss_angle  # å¤§å¹…æé«˜affordanceæƒé‡

            # æ¢¯åº¦ç´¯ç§¯: æ¯accumulation_stepsæ­¥æ‰æ›´æ–°å‚æ•°
            loss = loss / accumulation_steps  # æŸå¤±æŒ‰ç´¯ç§¯æ­¥æ•°ç¼©æ”¾
            scaler.scale(loss).backward()

            # æ¯accumulation_stepsæ­¥æ›´æ–°ä¸€æ¬¡å‚æ•°
            if (batch_idx + 1) % accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)

                # ä¼˜åŒ–å™¨æ­¥è¿›
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            train_loss += loss.item() * accumulation_steps  # æ¢å¤åŸå§‹æŸå¤±å€¼ç”¨äºè®°å½•

            # ç´¯ç§¯æŒ‡æ ‡ (åªåœ¨å‚æ•°æ›´æ–°æ—¶è®¡ç®—ï¼Œé¿å…é‡å¤è®¡ç®—)
            if (batch_idx + 1) % accumulation_steps == 0:
                # ç´¯ç§¯æŒ‡æ ‡
                batch_metrics = calculate_metrics(pred_afford, afford, pred_angle, angle, valid_mask)
                for k, v in batch_metrics.items():
                    train_metrics[k] += v

        # å¹³å‡è®­ç»ƒæŒ‡æ ‡ (è€ƒè™‘æ¢¯åº¦ç´¯ç§¯)
        num_effective_batches = len(train_loader) // accumulation_steps
        for k in train_metrics:
            train_metrics[k] /= max(1, num_effective_batches)  # é¿å…é™¤é›¶
        train_loss /= max(1, num_effective_batches)

        # ===== éªŒè¯é˜¶æ®µ =====
        model.eval()
        val_loss = 0.0
        val_metrics = {'afford_acc': 0, 'afford_precision': 0, 'afford_recall': 0, 'afford_f1': 0, 'angle_acc': 0}

        with torch.no_grad():
            for x, afford, angle, valid_mask in val_loader:
                x, afford, angle, valid_mask = x.cuda(), afford.cuda(), angle.cuda(), valid_mask.cuda()
                pred = model(x)
                pred_afford = pred[:, 0]
                pred_angle = pred[:, 1:]

                loss_afford_focal = criterion_afford_focal(pred_afford, afford)
                loss_afford_dice = criterion_afford_dice(pred_afford, afford)
                loss_afford = 0.7 * loss_afford_focal + 0.3 * loss_afford_dice

                if valid_mask.sum() > 0:
                    angle_loss_per_pixel = criterion_angle(pred_angle, angle)
                    masked_angle_loss = angle_loss_per_pixel * valid_mask
                    loss_angle = masked_angle_loss.sum() / (valid_mask.sum() + 1e-6)
                else:
                    loss_angle = torch.tensor(0.0).cuda()

                loss = 5.0 * loss_afford + 1.0 * loss_angle
                val_loss += loss.item()

                batch_metrics = calculate_metrics(pred_afford, afford, pred_angle, angle, valid_mask)
                for k, v in batch_metrics.items():
                    val_metrics[k] += v

        # å¹³å‡éªŒè¯æŒ‡æ ‡
        num_val_batches = len(val_loader)
        for k in val_metrics:
            val_metrics[k] /= num_val_batches
        val_loss /= num_val_batches

        # å­¦ä¹ ç‡è°ƒåº¦
        scheduler.step()

        # æ—¥å¿—è¾“å‡º
        print(f"[Epoch {epoch+1:3d}] LR: {optimizer.param_groups[0]['lr']:.6f} | "
              f"Train Loss: {train_loss:.4f} (Afford F1: {train_metrics['afford_f1']:.3f}, Angle Acc: {train_metrics['angle_acc']:.3f}) | "
              f"Val Loss: {val_loss:.4f} (Afford F1: {val_metrics['afford_f1']:.3f}, Angle Acc: {val_metrics['angle_acc']:.3f})")

        # ===== æ¨¡å‹ä¿å­˜ä¸æ—©åœ =====
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_counter = 0
            os.makedirs('./models', exist_ok=True)
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'val_loss': val_loss,
                'val_metrics': val_metrics,
                'train_metrics': train_metrics
            }, './models/affordance_model_best.pth')
            print("  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹")
        else:
            patience_counter += 1
            if patience_counter >= patience:
                print(f"  ğŸ›‘ æ—©åœ: {patience} ä¸ªepochéªŒè¯æŸå¤±æ— æ”¹å–„")
                break

    # ===== æœ€ç»ˆè¯„ä¼° =====
    print("\nğŸ“Š è®­ç»ƒå®Œæˆï¼åŠ è½½æœ€ä½³æ¨¡å‹è¿›è¡Œæœ€ç»ˆè¯„ä¼°...")
    checkpoint = torch.load('./models/affordance_model_best.pth')
    model.load_state_dict(checkpoint['model_state_dict'])

    model.eval()
    final_metrics = {'afford_acc': 0, 'afford_precision': 0, 'afford_recall': 0, 'afford_f1': 0, 'angle_acc': 0}
    with torch.no_grad():
        for x, afford, angle, valid_mask in val_loader:
            x, afford, angle, valid_mask = x.cuda(), afford.cuda(), angle.cuda(), valid_mask.cuda()
            pred = model(x)
            pred_afford = pred[:, 0]
            pred_angle = pred[:, 1:]

            batch_metrics = calculate_metrics(pred_afford, afford, pred_angle, angle, valid_mask)
            for k, v in batch_metrics.items():
                final_metrics[k] += v

    for k in final_metrics:
        final_metrics[k] /= len(val_loader)

    print("=" * 70)
    print("ğŸ¯ æœ€ç»ˆéªŒè¯é›†æ€§èƒ½:")
    print(f"Afford Acc: {final_metrics['afford_acc']:.3f}")
    print(f"Afford Precision: {final_metrics['afford_precision']:.3f}")
    print(f"Afford Recall: {final_metrics['afford_recall']:.3f}")
    print(f"Afford F1: {final_metrics['afford_f1']:.3f}")
    print(f"Angle Acc: {final_metrics['angle_acc']:.3f}")
    print("=" * 70)
    print("ğŸ’¡ æ–¹æ³•è®ºå…³é”®ç‚¹:")
    print("   â€¢ è‡ªç›‘ç£æ•°æ®ç”Ÿæˆ: ç‰©ç†ä»¿çœŸæä¾›æ ‡ç­¾")
    print("   â€¢ Focal Loss: å¤„ç†æåº¦ä¸å¹³è¡¡çš„affordanceæ ‡ç­¾")
    print("   â€¢ Masked Angle Loss: åªåœ¨å¯æŠ“å–åƒç´ ä¸Šè®¡ç®—è§’åº¦æŸå¤±")
    print("   â€¢ å¤šä»»åŠ¡å­¦ä¹ : affordance(3xæƒé‡) + angle(0.8xæƒé‡)")
    print("   â€¢ ä½™å¼¦é€€ç«è°ƒåº¦: æ›´å¥½çš„æ”¶æ•›å’Œæ³›åŒ–")


if __name__ == '__main__':
    train_model()
