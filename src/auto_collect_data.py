#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è‡ªåŠ¨åŒ–å¯ä¾›æ€§è®­ç»ƒæ•°æ®æ”¶é›†å™¨
Automatic Affordance Training Data Collector

åŸºäºå·²éªŒè¯çš„å·¥ä½œç³»ç»Ÿï¼Œè‡ªåŠ¨æ”¶é›†è®­ç»ƒæ•°æ®
"""

import numpy as np
import cv2
import os
import json
import time
import argparse
import pybullet as p
from PIL import Image
from pathlib import Path
import sys
import matplotlib.pyplot as plt

# å¯¼å…¥å·¥ä½œçš„æ¨¡å—
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))
from src.environment_setup import setup_environment
from src.perception import set_topdown_camera, get_rgb_depth_segmentation, pixel_to_world
from src.afford_data_gen import sample_grasp_candidates, fast_grasp_test, reset_robot_home

class AutoAffordanceCollector:
    """è‡ªåŠ¨åŒ–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨"""
    
    def __init__(self, data_dir="data/affordance_v5", num_angles=8, train_split=0.8):
        # Convert to absolute path relative to the script's parent directory (workspace root)
        if not Path(data_dir).is_absolute():
            script_dir = Path(__file__).parent
            workspace_root = script_dir.parent
            data_dir = workspace_root / data_dir
        
        self.data_dir = Path(data_dir)
        self.data_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•å­ç›®å½•
        self.train_dir = self.data_dir / "train"
        self.test_dir = self.data_dir / "test"
        self.train_dir.mkdir(exist_ok=True)
        self.test_dir.mkdir(exist_ok=True)
        
        self.num_angles = num_angles
        self.train_split = train_split  # è®­ç»ƒé›†æ¯”ä¾‹ (0.8 = 80%)
        
        # æŠ“å–è§’åº¦è®¾ç½®
        self.grasp_angles = np.linspace(0, 2*np.pi, num_angles, endpoint=False)
        
        print(f"ğŸ¯ è‡ªåŠ¨åŒ–å¯ä¾›æ€§æ•°æ®æ”¶é›†å™¨åˆå§‹åŒ–")
        print(f"   æ•°æ®ç›®å½•: {self.data_dir.absolute()}")
        print(f"   è®­ç»ƒç›®å½•: {self.train_dir}")
        print(f"   æµ‹è¯•ç›®å½•: {self.test_dir}")
        print(f"   è®­ç»ƒé›†æ¯”ä¾‹: {train_split:.1%}")
        print(f"   æŠ“å–è§’åº¦æ•°: {num_angles}")
    
    def collect_single_attempt_as_scene(self, scene_idx, robot_id, object_ids, target_dir):
        """æ”¶é›†å•æ¬¡æŠ“å–å°è¯•ä½œä¸ºç‹¬ç«‹åœºæ™¯ï¼Œæ¯æ¬¡æŠ“å–å‰æ¢å¤ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€"""
        print(f"============================================================")
        
        # 1. è®°å½•åˆå§‹æœºå™¨äººå’Œç‰©ä½“çŠ¶æ€
        home_joints = [0, -0.785, 0, -2.356, 0, 1.571, 0.785]
        initial_robot_joints = [p.getJointState(robot_id, i)[0] for i in range(7)]
        initial_gripper = [p.getJointState(robot_id, 9)[0], p.getJointState(robot_id, 10)[0]]
        object_states = {}
        for obj_id in object_ids:
            pos, orn = p.getBasePositionAndOrientation(obj_id)
            object_states[obj_id] = (pos, orn)
        
        # 2. ç¡®ä¿æœºå™¨äººåœ¨åˆå§‹ä½ç½®
        print("   ğŸ  é‡ç½®æœºå™¨äºº...")
        for i in range(7):
            p.setJointMotorControl2(
                robot_id, i, p.POSITION_CONTROL,
                targetPosition=home_joints[i], 
                force=500, 
                maxVelocity=2.0
            )
        for _ in range(200):
            p.stepSimulation()
            all_in_position = True
            for i in range(7):
                current = p.getJointState(robot_id, i)[0]
                if abs(current - home_joints[i]) > 0.05:
                    all_in_position = False
                    break
            if all_in_position:
                break
        # å¼ºåˆ¶æ‰“å¼€å¤¹çˆª
        p.setJointMotorControl2(robot_id, 9, p.POSITION_CONTROL, targetPosition=0.02, force=300)
        p.setJointMotorControl2(robot_id, 10, p.POSITION_CONTROL, targetPosition=0.02, force=300)
        for _ in range(40):
            p.stepSimulation()
        # éªŒè¯æœºå™¨äººä½ç½®
        current_pos = p.getLinkState(robot_id, 8)[0]
        print(f"   ğŸ“ æœºå™¨äººæœ«ç«¯ä½ç½®: [{current_pos[0]:.3f}, {current_pos[1]:.3f}, {current_pos[2]:.3f}]")
        # æ‹æ‘„ç…§ç‰‡
        print("   ğŸ“· æ‹æ‘„åœºæ™¯ç…§ç‰‡...")
        width, height, view_matrix, proj_matrix = set_topdown_camera()
        rgb_image, depth_image, seg_mask = get_rgb_depth_segmentation(width, height, view_matrix, proj_matrix)
        print(f"   ğŸ“· ç›¸æœºæ•°æ®: RGB {rgb_image.shape}, æ·±åº¦ {depth_image.shape}")
        # é‡‡æ ·æŠ“å–å€™é€‰ç‚¹
        candidates = sample_grasp_candidates(
            depth=depth_image,
            num_angles=self.num_angles,
            visualize=False,
            rgb=rgb_image,
            view_matrix=view_matrix,
            proj_matrix=proj_matrix,
            seg_mask=seg_mask,
            object_ids=object_ids
        )
        
        print(f"   ğŸ” å€™é€‰ç‚¹é‡‡æ ·ç»“æœ: {len(candidates)} ä¸ªå€™é€‰ç‚¹")
        
        if not candidates:
            print("   âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æŠ“å–å€™é€‰ç‚¹")
            return False
        
        print(f"   ğŸ“ é‡‡æ ·äº† {len(candidates)} ä¸ªå€™é€‰ç‚¹ - æµ‹è¯•å‰ {min(20, len(candidates))} ä¸ª")
        
        # æµ‹è¯•å¤šä¸ªå€™é€‰ç‚¹ï¼ˆæ¯ä¸ªåœºæ™¯æœ€å¤š20ä¸ªï¼Œæä¾›æ›´å¯†é›†çš„è®­ç»ƒæ ‡ç­¾ï¼‰
        test_count = min(20, len(candidates))
        successful_grasps = []
        failed_grasps = []
        
        for i, candidate in enumerate(candidates[:test_count]):
            if len(candidate) == 4:
                u, v, theta_idx, theta = candidate
            else:
                u, v, theta_idx = candidate
                theta = self.grasp_angles[theta_idx]
            world_pos = pixel_to_world(u, v, depth_image[v, u], view_matrix, proj_matrix)
            print(f"   ğŸ¯ æµ‹è¯• {i+1}/{test_count}: åƒç´ ({u},{v}), è§’åº¦{np.degrees(theta):.1f}Â°")
            # æ¯æ¬¡æŠ“å–å‰æ¢å¤æœºå™¨äººå’Œç‰©ä½“åˆ°åˆå§‹çŠ¶æ€
            if i > 0:
                print(f"      ğŸ  æ¢å¤æœºå™¨äººå’Œç‰©ä½“åˆ°åˆå§‹çŠ¶æ€...")
                # æ¢å¤æœºå™¨äººå…³èŠ‚
                for j in range(7):
                    p.resetJointState(robot_id, j, home_joints[j])
                # æ¢å¤å¤¹çˆª
                p.resetJointState(robot_id, 9, 0.02)
                p.resetJointState(robot_id, 10, 0.02)
                # æ¢å¤ç‰©ä½“
                for obj_id in object_ids:
                    pos, orn = object_states[obj_id]
                    p.resetBasePositionAndOrientation(obj_id, pos, orn)
                # è®©ç‰©ç†å¼•æ“ç¨³å®š
                for _ in range(10):
                    p.stepSimulation()
            # æ‰§è¡ŒæŠ“å–æµ‹è¯•
            success = fast_grasp_test(robot_id, world_pos, theta, object_ids, visualize=False)
            if success:
                print(f"      âœ… æˆåŠŸ!")
                successful_grasps.append({'pixel': (u, v), 'angle': theta, 'world_pos': world_pos})
            else:
                print(f"      âŒ å¤±è´¥")
                failed_grasps.append({'pixel': (u, v), 'angle': theta, 'world_pos': world_pos})
        
        # è®¡ç®—æˆåŠŸç‡
        success_rate = len(successful_grasps) / test_count if test_count > 0 else 0
        print(f"   ğŸ“Š åœºæ™¯æˆåŠŸç‡: {len(successful_grasps)}/{test_count} ({success_rate:.1%})")
        
        # ç”Ÿæˆå¯ä¾›æ€§æ ‡ç­¾ï¼ˆæ ‡è®°æ‰€æœ‰æµ‹è¯•çš„ç‚¹ï¼‰
        affordance_map = np.zeros((224, 224), dtype=np.float32)
        angle_map = np.zeros((224, 224), dtype=np.float32) 
        
        # æ ‡è®°æˆåŠŸçš„æŠ“å–ç‚¹
        for grasp in successful_grasps:
            u, v = grasp['pixel']
            theta = grasp['angle']
            affordance_map[v, u] = 1.0  # æˆåŠŸç‚¹æ ‡è®°ä¸º1
            angle_map[v, u] = theta
        
        # å¤±è´¥çš„ç‚¹ä¿æŒä¸º0ï¼ˆå·²ç»åˆå§‹åŒ–ä¸º0ï¼‰
        
        # åˆå¹¶æ‰€æœ‰æŠ“å–ç»“æœ
        all_results = []
        for grasp in successful_grasps + failed_grasps:
            all_results.append({
                'success': grasp in successful_grasps,
                'pixel': grasp['pixel'],
                'world_pos': grasp['world_pos'],
                'angle': grasp['angle']
            })
        
        # ä½¿ç”¨ç»Ÿä¸€çš„ä¿å­˜æ–¹æ³•
        self.save_scene_data(scene_idx, rgb_image, depth_image, affordance_map, angle_map, all_results, view_matrix, proj_matrix, target_dir, robot_id=robot_id)
        
        return True
        
    def is_position_reachable(self, world_pos):
        """æ£€æŸ¥ä½ç½®æ˜¯å¦åœ¨æœºå™¨äººå·¥ä½œç©ºé—´å†…"""
        x, y, z = world_pos
        
        # åŸºæœ¬å·¥ä½œç©ºé—´é™åˆ¶
        distance = np.sqrt(x**2 + y**2)
        
        # Franka Pandaçš„å·¥ä½œç©ºé—´çº¦æŸ
        if distance < 0.3 or distance > 0.85:  # è·ç¦»é™åˆ¶
            return False
        if abs(y) > 0.4:  # Yè½´é™åˆ¶
            return False
        if z < 0.58 or z > 0.8:  # Zè½´é«˜åº¦é™åˆ¶
            return False
        if x < 0.2 or x > 0.9:  # Xè½´å‰åé™åˆ¶
            return False
            
        return True
    
        """ç”Ÿæˆå¯ä¾›æ€§çƒ­åŠ›å›¾"""
    def generate_angle_map(self, results, image_shape):
        """ç”Ÿæˆæœ€ä½³æŠ“å–è§’åº¦åœ°å›¾"""
        angle_map = np.zeros(image_shape, dtype=np.float32)
        
        for result in results:
            if result['success'] and result['world_pos'][0] != 0:
                u, v = result['pixel']
                if 0 <= v < image_shape[0] and 0 <= u < image_shape[1]:
                    # å½’ä¸€åŒ–è§’åº¦åˆ° [0, 1]
                    normalized_angle = result['angle'] / (2 * np.pi)
                    angle_map[v, u] = normalized_angle
        
        return angle_map
    
    def create_affordance_map(self, image_shape, results):
        """åˆ›å»ºå¯ä¾›æ€§åœ°å›¾"""
        height, width = image_shape
        affordance_map = np.zeros((height, width), dtype=np.float32)
        
        for result in results:
            if 'pixel' in result and len(result['pixel']) == 2:
                u, v = result['pixel']
                if 0 <= v < height and 0 <= u < width:
                    affordance_map[v, u] = 1.0 if result['success'] else 0.0
        
        return affordance_map
    
    def create_angle_map(self, image_shape, results):
        """åˆ›å»ºè§’åº¦åœ°å›¾"""
        height, width = image_shape
        angle_map = np.zeros((height, width), dtype=np.float32)
        
        for result in results:
            if result['success'] and result['world_pos'][0] != 0:
                u, v = result['pixel']
                if 0 <= v < height and 0 <= u < width:
                    angle_map[v, u] = result['angle']
        
        return angle_map
    
    def save_scene_data(self, scene_id, rgb_image, depth_image, affordance_map, angle_map, results, view_matrix, proj_matrix, target_dir, seg_mask=None, robot_id=None):
        """ä¿å­˜åœºæ™¯æ•°æ®"""
        scene_prefix = f"scene_{scene_id:04d}"
        
        # ä¿å­˜å›¾åƒ
        rgb_path = target_dir / f"{scene_prefix}_rgb.png"
        cv2.imwrite(str(rgb_path), cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))
        
        # è½¬æ¢æ·±åº¦å›¾ä¸ºç›¸å¯¹æ·±åº¦
        # ä½¿ç”¨ç²¾ç¡®çš„å·¥ä½œç©ºé—´mask (åŸºäºä¸–ç•Œåæ ‡)
        height, width = depth_image.shape
        workspace_mask = np.zeros((height, width), dtype=bool)
        edge_mask = np.zeros((height, width), dtype=bool)
        
        # ä¸ºæ¯ä¸ªåƒç´ è®¡ç®—ä¸–ç•Œåæ ‡å¹¶æ£€æŸ¥æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…
        TABLE_TOP_Z = 0.625  # ä»environment_setup.py
        for v in range(height):
            for u in range(width):
                # å°†åƒç´ åæ ‡è½¬æ¢ä¸ºä¸–ç•Œåæ ‡
                world_pos = pixel_to_world(u, v, depth_image[v, u], view_matrix, proj_matrix)
                x, y, z = world_pos
                
                # æ£€æŸ¥æ˜¯å¦åœ¨å·¥ä½œç©ºé—´å†…
                dist_from_base = np.sqrt(x**2 + y**2)
                in_workspace = (
                    z >= TABLE_TOP_Z and z <= TABLE_TOP_Z + 0.25 and  # ZèŒƒå›´æ›´ä¿å®ˆ
                    dist_from_base >= 0.35 and dist_from_base <= 0.80 and  # è·ç¦»èŒƒå›´æ›´ä¿å®ˆ
                    abs(y) <= 0.30  # Yè½´èŒƒå›´æ›´ä¸¥æ ¼ï¼Œæ’é™¤æœºå™¨äººåŸºåº§
                )
                
                workspace_mask[v, u] = in_workspace
                
                # å®šä¹‰è¾¹ç¼˜åŒºåŸŸï¼šåªåœ¨è¾¹ç¼˜é‡‡æ ·æ¡Œé¢æ·±åº¦ï¼Œé¿å…ç‰©ä½“å¹²æ‰°
                if in_workspace and (dist_from_base <= 0.45 or dist_from_base >= 0.70):
                    edge_mask[v, u] = True
        
        # åªä½¿ç”¨å·¥ä½œç©ºé—´è¾¹ç¼˜çš„åƒç´ è®¡ç®—æ¡Œé¢æ·±åº¦ï¼Œé¿å…ç‰©ä½“å¹²æ‰°
        edge_depths = depth_image[edge_mask]
        
        if len(edge_depths) == 0:
            print("      âš ï¸ è­¦å‘Š: è¾¹ç¼˜åŒºåŸŸæ²¡æœ‰æœ‰æ•ˆåƒç´ ï¼Œä½¿ç”¨å…¨å·¥ä½œç©ºé—´")
            edge_depths = depth_image[workspace_mask]
            if len(edge_depths) == 0:
                edge_depths = depth_image.flatten()
        
        # ä½¿ç”¨è¾¹ç¼˜åŒºåŸŸå†…æ·±åº¦æœ€ä½çš„åƒç´ ä½œä¸ºæ¡Œé¢
        sorted_depths = np.sort(edge_depths)
        num_table_candidates = max(50, len(sorted_depths) // 5)
        table_pixels = sorted_depths[:num_table_candidates]
        table_depth = np.percentile(table_pixels, 10)
        
        print(f"      ğŸ“ save_scene_data: ä½¿ç”¨å·¥ä½œç©ºé—´è¾¹ç¼˜ {num_table_candidates} ä¸ªæ·±åº¦åƒç´ è®¡ç®—æ¡Œé¢æ·±åº¦: {table_depth:.6f}")
        
        # åˆ›å»ºç›¸å¯¹æ·±åº¦å›¾ï¼šå·¥ä½œç©ºé—´å†…åƒç´ ä½¿ç”¨å®é™…ç›¸å¯¹æ·±åº¦ï¼Œå·¥ä½œç©ºé—´å¤–åƒç´ è®¾ä¸º0ï¼ˆæ¡Œé¢æ·±åº¦ï¼‰
        relative_depth = np.zeros_like(depth_image)
        relative_depth[workspace_mask] = depth_image[workspace_mask] - table_depth
        # å·¥ä½œç©ºé—´å¤–åƒç´ ä¿æŒä¸º0ï¼ˆç›¸å½“äºæ¡Œé¢æ·±åº¦ï¼‰
        
        # ä¿å­˜æ·±åº¦å›¾ï¼ˆç›¸å¯¹æ·±åº¦ï¼‰
        depth_path = target_dir / f"{scene_prefix}_depth.npy"
        np.save(depth_path, relative_depth)
        
        # ä¿å­˜å¯ä¾›æ€§åœ°å›¾
        affordance_path = target_dir / f"{scene_prefix}_affordance.npy"
        np.save(affordance_path, affordance_map)
        
        # ä¿å­˜è§’åº¦åœ°å›¾
        angle_path = target_dir / f"{scene_prefix}_angles.npy"
        np.save(angle_path, angle_map)
        
        # ä¿å­˜å…ƒæ•°æ®
        metadata = {
            'scene_id': int(scene_id),
            'image_shape': [int(x) for x in rgb_image.shape[:2]],
            'num_candidates': len(results),
            'num_successful': sum(1 for r in results if r['success']),
            'success_rate': float(sum(1 for r in results if r['success']) / len(results)) if results else 0.0,
            'candidates': [{
                'success': bool(r['success']),
                'pixel': [int(r['pixel'][0]), int(r['pixel'][1])],
                'world_pos': [float(r['world_pos'][0]), float(r['world_pos'][1]), float(r['world_pos'][2])],
                'angle': float(r['angle'])
            } for r in results]
        }
        
        meta_path = target_dir / f"{scene_prefix}_meta.json"
        with open(meta_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"      ğŸ’¾ æ•°æ®å·²ä¿å­˜: {scene_prefix}_*")
    
    def visualize_affordance_map(self, affordance_map, scene_id):
        """ä½¿ç”¨jet colormapå¯è§†åŒ–å¯ä¾›æ€§åœ°å›¾"""
        plt.figure(figsize=(8, 6))
        plt.imshow(affordance_map, cmap='jet', vmin=0, vmax=1)
        plt.title(f'Affordance Map - Scene {scene_id:04d}')
        plt.colorbar()
        plt.axis('off')
        plt.tight_layout()
        
        # ä¿å­˜å¯è§†åŒ–å›¾åƒ
        vis_path = target_dir / f"scene_{scene_id:04d}_affordance_vis.png"
        plt.savefig(str(vis_path), dpi=150, bbox_inches='tight')
        plt.close()
        print(f"      ğŸ“Š å¯ä¾›æ€§åœ°å›¾å¯è§†åŒ–å·²ä¿å­˜: {vis_path}")
    
    def collect_dataset(self, num_scenes, num_objects_range=(2, 4), max_attempts_per_scene=25):
        """æ”¶é›†å®Œæ•´æ•°æ®é›†"""
        print(f"\nğŸš€ å¼€å§‹æ”¶é›†æ•°æ®é›†")
        print(f"   åœºæ™¯æ•°é‡: {num_scenes}")
        print(f"   ç‰©ä½“æ•°é‡èŒƒå›´: {num_objects_range[0]}-{num_objects_range[1]}")
        print(f"   æ¯åœºæ™¯æœ€å¤§å°è¯•æ•°: {max_attempts_per_scene}")
        print("=" * 60)
        
        successful_scenes = 0
        total_grasps = 0
        total_successful_grasps = 0
        
        for scene_id in range(num_scenes):
            # âœ¨ æ¯ä¸ªåœºæ™¯éƒ½åˆ›å»ºæ–°çš„ç¯å¢ƒé…ç½®ï¼Œç¡®ä¿åœºæ™¯å¤šæ ·æ€§
            # æ¸…ç†ä¹‹å‰çš„ç¯å¢ƒ
            if scene_id > 0:
                for obj_id in object_ids:
                    try:
                        p.removeBody(obj_id)
                    except:
                        pass
            
            # éšæœºé€‰æ‹©ç‰©ä½“æ•°é‡å¹¶åˆ›å»ºæ–°ç¯å¢ƒ
            num_objects = np.random.randint(num_objects_range[0], num_objects_range[1] + 1)
            
            # âœ¨ ä¸ºæ¯ä¸ªåœºæ™¯è®¾ç½®ä¸åŒçš„éšæœºç§å­ï¼Œç¡®ä¿ç‰©ä½“ä½ç½®ä¸åŒ
            np.random.seed(scene_id + int(time.time()) % 1000)
            
            print(f"ğŸ—ï¸  åˆ›å»ºæ–°ç¯å¢ƒé…ç½® #{scene_id + 1}: {num_objects} ä¸ªç‰©ä½“")
            robot_id, object_ids = setup_environment(num_objects=num_objects)
            if not object_ids:
                print(f"   âŒ ç¯å¢ƒè®¾ç½®å¤±è´¥ï¼Œè·³è¿‡åœºæ™¯ {scene_id}")
                continue
            
            # è°ƒè¯•: æ‰“å°ç‰©ä½“ä½ç½®å’Œæœºå™¨äººå§¿æ€
            print(f"ğŸ” åœºæ™¯ {scene_id} ç¯å¢ƒè®¾ç½®è°ƒè¯•:")
            robot_pos = p.getLinkState(robot_id, 8)[0]
            print(f"  ğŸ“ æœºå™¨äººæœ«ç«¯ä½ç½®: {robot_pos}")
            print(f"  ğŸ“ ç‰©ä½“ä½ç½®:")
            for i, obj_id in enumerate(object_ids):
                pos, orn = p.getBasePositionAndOrientation(obj_id)
                print(f"    ç‰©ä½“ {i}: ä½ç½®={pos}, æœå‘={orn}")
            
            # æ”¶é›†å•æ¬¡æŠ“å–å°è¯•ä½œä¸ºç‹¬ç«‹åœºæ™¯
            print(f"   ğŸ” åœºæ™¯ {scene_id} å¼€å§‹ - ç‰©ä½“IDs: {object_ids}")
            
            # éšæœºå†³å®šè¯¥åœºæ™¯å±äºè®­ç»ƒé›†è¿˜æ˜¯æµ‹è¯•é›†
            is_train = np.random.random() < self.train_split
            target_dir = self.train_dir if is_train else self.test_dir
            split_name = "è®­ç»ƒé›†" if is_train else "æµ‹è¯•é›†"
            print(f"   ğŸ“Š åœºæ™¯åˆ†é…: {split_name} ({target_dir.name})")
            
            success = self.collect_single_attempt_as_scene(scene_id, robot_id, object_ids, target_dir)
            
            # è¯»å–åœºæ™¯ç»Ÿè®¡ä¿¡æ¯
            import json
            meta_file = target_dir / f"scene_{scene_id:04d}_meta.json"
            if meta_file.exists():
                with open(meta_file, 'r') as f:
                    meta = json.load(f)
                    scene_attempts = meta.get('num_candidates', 0)
                    scene_successes = meta.get('num_successful', 0)
                    total_grasps += scene_attempts
                    total_successful_grasps += scene_successes
            
            # åœºæ™¯é—´çŸ­æš‚åœé¡¿ï¼Œè®©ç‰©ç†å¼•æ“ç¨³å®š
            if scene_id < num_scenes - 1:
                for _ in range(10):
                    p.stepSimulation()
                    time.sleep(1./240.)
            
            if success:
                successful_scenes += 1
                print(f"   âœ… åœºæ™¯ {scene_id} å®Œæˆ (æœ‰å¯ç”¨æ•°æ®)")
            else:
                print(f"   âŒ åœºæ™¯ {scene_id} å®Œæˆ (æ— å¯ç”¨æ•°æ®)")
        
        # æ€»ç»“
        grasp_success_rate = (total_successful_grasps / total_grasps) if total_grasps > 0 else 0
        print("=" * 60)
        print(f"ğŸ‰ æ•°æ®æ”¶é›†å®Œæˆ!")
        print(f"   æœ‰æ•ˆåœºæ™¯: {successful_scenes}/{num_scenes}")
        print(f"   æ€»æŠ“å–æˆåŠŸç‡: {grasp_success_rate:.1%} ({total_successful_grasps}/{total_grasps})")
        print(f"   æ•°æ®ä¿å­˜ä½ç½®:")
        print(f"     è®­ç»ƒé›†: {self.train_dir} ({self.train_split:.1%})")
        print(f"     æµ‹è¯•é›†: {self.test_dir} ({1-self.train_split:.1%})")
        print("âœ… æ•°æ®æ”¶é›†æˆåŠŸ!")
        
        return successful_scenes > 0

def main():
    parser = argparse.ArgumentParser(description='è‡ªåŠ¨åŒ–å¯ä¾›æ€§è®­ç»ƒæ•°æ®æ”¶é›†å™¨')
    parser.add_argument('--num_scenes', type=int, default=10, help='æ”¶é›†çš„åœºæ™¯æ•°é‡')
    parser.add_argument('--num_objects', type=int, nargs=2, default=[2, 4], help='ç‰©ä½“æ•°é‡èŒƒå›´ [min, max]')
    parser.add_argument('--max_attempts', type=int, default=25, help='æ¯åœºæ™¯æœ€å¤§æŠ“å–å°è¯•æ•°')
    parser.add_argument('--data_dir', type=str, default='data/affordance_v5', help='æ•°æ®ä¿å­˜ç›®å½•')
    parser.add_argument('--angles', type=int, default=8, help='ç¦»æ•£æŠ“å–è§’åº¦æ•°é‡')
    parser.add_argument('--train_split', type=float, default=0.8, help='è®­ç»ƒé›†æ¯”ä¾‹ (0.0-1.0)')
    parser.add_argument('--gui', action='store_true', help='æ˜¾ç¤ºGUI')
    
    args = parser.parse_args()
    
    # è¿æ¥PyBullet
    if args.gui:
        p.connect(p.GUI)
    else:
        p.connect(p.DIRECT)
    
    try:
        # åˆ›å»ºæ”¶é›†å™¨
        collector = AutoAffordanceCollector(
            data_dir=args.data_dir,
            num_angles=args.angles,
            train_split=args.train_split
        )
        
        # æ”¶é›†æ•°æ®
        success = collector.collect_dataset(
            num_scenes=args.num_scenes,
            num_objects_range=tuple(args.num_objects),
            max_attempts_per_scene=args.max_attempts
        )
        
        if success:
            print("âœ… æ•°æ®æ”¶é›†æˆåŠŸ!")
        else:
            print("âŒ æ•°æ®æ”¶é›†å¤±è´¥!")
    
    finally:
        p.disconnect()

if __name__ == "__main__":
    main()